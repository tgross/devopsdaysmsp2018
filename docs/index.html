<!doctype html><html lang="en">
  <head>
	<meta name="generator" content="Hugo 0.43" />
    <meta charset="utf-8">
    <title>DevOps Days MSP 2018</title>
    <meta name="description" content="">
    <meta name="author" content="">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <link rel="stylesheet" href="./css/reveal.min.css">
    
    
    
    
    <link rel="stylesheet" href="./css/devopsdays.css" id="theme">
    
    
    
    <link rel="stylesheet" href="./styles/solarized-dark.min.css">
    
    <script>
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = window.location.search.match( /print-pdf/gi ) ? './css/print/pdf.css' : './css/print/paper.css';
      document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>
    
    
  </head>
  <body>
    
  <div class="reveal">
  <div class="footer"><a href="https://twitter.com/0x74696d">@0x74696d</a></div>

  
  <div class="slides">
    
    <section>
              

<h1 id="they-didn-t-stop-to-think-if-they-should">They Didn&rsquo;t Stop To Think If They Should</h1>

<h4 id="tim-gross-machinist-labs">Tim Gross | Machinist Labs</h4>

<aside class="notes"><p>I&rsquo;m Tim Gross. This is my twitter handle, which seemed like a good idea at the time. &ldquo;They Didn&rsquo;t Stop to Think If They Should&rdquo;</p>
</aside>


            </section>
          <section>
              

<h2 id="or">or&hellip;</h2>

<h2 id="machine-learning-and-the-internet-of-unpatched-things">Machine Learning And The Internet Of Unpatched Things</h2>

<aside class="notes"><p>an alternate title could be&hellip; &ldquo;Machine Learning And The Internet Of Unpatched Things&rdquo;</p>
</aside>


            </section>
          <section>
              

<h2 id="or-1">or&hellip;</h2>

<h2 id="because-eternal-vigilance-is-the-price-of-liberty-we-have-to-talk-about-ethics-again">Because Eternal Vigilance is the Price of Liberty, We Have to Talk About Ethics Again</h2>

<aside class="notes"><p>or maybe&hellip; &ldquo;Because Eternal Vigilance is the Price of Liberty, We Have to Talk About Ethics Again&rdquo;</p>
</aside>


            </section>
          <section>
              

<blockquote>
<p>We&rsquo;ve updated our privacy policy!</p>

<p>Any reference to or citation of any person or organization does not constitute or imply an endorsement or recommendation of the content of this talk. The opinions expressed in this talk are the speaker&rsquo;s alone and do not reflect the view of this conference, your employer, or my mom. The speaker is grossly unqualified to tell you how to live your life. Your mileage may vary. Not to be used in the manufacture of nuclear weapons. By attending this talk the speaker hereby grants you an irrevocable, perpetual, non-exclusive, transferable, worldwide license to be excellent to each other.</p>
</blockquote>

<aside class="notes"><p>&ldquo;Is this guy gonna talk about the trolley problem? C&rsquo;mon, I just want to code! And who is this guy, anyways? He&rsquo;s not an ethicist with a PhD!&rdquo; Why do we as engineers need to discuss this stuff? And why is it especially important to talk about ethics when we start talking about ML and IoT? What makes these things special? What makes them different from other software? the reason we <em>all</em> need to be talking about this, is because as Sharon Kennedy Vickers told us yesterday: &ldquo;what we do has impact&rdquo;</p>
</aside>


            </section>
          <section>
              

<p><img src="img/uber-accident.png" alt="Washington Post: NTSB finds self-driving Uber did not have emergency braking turned on" /></p>

<p><em><a href="https://www.washingtonpost.com/news/dr-gridlock/wp/2018/05/24/ntsb-self-driving-uber-did-not-have-emergency-braking-turned-on/">Michael Laris, Washington Post, 24 May 2018</a></em></p>

<aside class="notes"><p>Back in March a self-driving car operated by Uber killed a pedestrian. The NTSB investigation is still ongoing, but what is clear is that the vehicle had no business being operated without direct supervision. The collision detection system had lots of false positives that caused it to brake erratically. So they turned the braking system off. But apparently that message never got to the folks who&rsquo;d turned off the vehicles&rsquo; own automatic braking nor to the attendee of the vehicle. So the vehicle &ldquo;saw&rdquo; the pedestrian and did nothing.</p>
</aside>


            </section>
          <section>
              

<p><img src="img/microsoft-chatbot.png" alt="The Verge: Twitter taught Microsoft's Tay Chatbot to be a racist asshole in less than a day" /></p>

<p><em><a href="https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist">James Vincent, The Verge, 24 Mar 2016</a></em></p>

<aside class="notes"><p>A couple years ago, MSFT demonstrated their ML capabilities with the Tay chatbot. They let it loose to learn from whatever racist trolls wanted to teach it. The experiment was stopped and the whole debacle embarrassed MSFT. Now clearly the horrible Internet people are primarily to blame here, but the researchers failed to anticipate the vulnerabilities inherent to their system</p>
</aside>


            </section>
          <section>
              

<p><img src="img/uk-ransomware.png" alt="The Verge: UK hospitals hit with massive ransomware attack" /></p>

<p><em><a href="https://www.theverge.com/2017/5/12/15630354/nhs-hospitals-ransomware-hack-wannacry-bitcoin">Russell Brandom, The Verge, 12 May 2017</a></em></p>

<aside class="notes"><p>in an IoT example, last year countless devices in UK hospitals were taken over by Wannacry ransomware. patient lives were put at risk not just because of poor patch hygiene but because of the architectural flaws and because of warped incentives in the US government program that produced the malware in the first place.</p>
</aside>


            </section>
          <section>
              

<p><img src="img/tesla-autopilot.png" alt="The Guardian: Tesla car that crashed and killed driver was running on Autopilot, firm says" /></p>

<p><em><a href="https://www.theguardian.com/technology/2018/mar/31/tesla-car-crash-autopilot-mountain-view">The Guardian, 31 Mar 2018</a></em></p>

<aside class="notes"><p>Tesla&rsquo;s autopilot has been implicated in the deaths of several drivers at this point. We (and Tesla stockholders, apparently) are constantly reassured that these are the result of improper handling and not rushing these systems into the real world before they&rsquo;re ready.</p>
</aside>


            </section>
          <section>
              

<p><img src="img/target-pregnant.png" alt="Forbes: How Target Figured Out a Teen Girl Was Pregnant Before Her Father Did" /></p>

<p><em><a href="https://www.forbes.com/sites/kashmirhill/2012/02/16/how-target-figured-out-a-teen-girl-was-pregnant-before-her-father-did/">Kashmir Hill, Forbes, 16 Feb 2012</a></em></p>

<aside class="notes"><p>You didn&rsquo;t think I was going to leave the home team out? Machine learning algorithms may know more about us and our loved ones than we do ourselves. Target was able to determine from customer purchases not just when customers are pregnant but at what stage of their pregnancy they were (buying unscented products during late stages, for example).</p>
</aside>


            </section>
          <section>
              

<p><img src="img/google-racist-autotag.png" alt="The Guardian: Google says sorry for racist auto-tag in photo app" /></p>

<p><em><a href="https://www.theguardian.com/technology/2015/jul/01/google-sorry-racist-auto-tag-photo-app">Jana Kasperkevic, The Guardian, 1 Jul 2015</a></em></p>

<aside class="notes"><p>ML is encoding our biases. (This one was so bad that I cropped it out of this picture.)</p>
</aside>


            </section>
          <section>
              

<p><img src="img/defeat-the-baby.png" alt="@yipe on Twitter: &quot;Alexa: remind me to feed the baby&quot; (response: &quot;Defeat the baby&quot;)" /></p>

<p><em><a href="https://twitter.com/yipe/status/1005555741153902592">https://twitter.com/yipe/status/1005555741153902592</a></em></p>

<aside class="notes"><p>we&rsquo;ve invited these systems into our homes&hellip; (and ok, yeah this is funny)</p>
</aside>


            </section>
          <section>
              

<p><img src="img/alexa-eavesdropping.png" alt="Huffington Post: Amazon Alexa-Powered Device Recorded and Shared User's Conversation Without Permission" /></p>

<p><em><a href="https://www.huffingtonpost.com/entry/alexa-eavesdropping-portland-familiy_us_5b0727cae4b0fdb2aa51b23e">Laura Stevens, Huffington Post, 24 May 2018</a></em></p>

<aside class="notes"><p>&hellip; with almost no oversight into what they&rsquo;re doing. We have unaccountable closed source microphones in our homes, who do who-knows-what with that information</p>
</aside>


            </section>
          <section>
              

<p><img src="img/marco-rogers.png" alt="@polotek on Twitter: &quot;Kalanick keeps asking for unethical/illegal things, but at some point we have to talk about how engineers at Uber keep saying yes.&quot;" /></p>

<aside class="notes"><p>what all these stories have in common is that like any other failure there&rsquo;s almost certainly no &ldquo;root cause&rdquo;. these failures are the result of complex sociotechnical systems and the incentives they set up. it&rsquo;s unlikely that anyone at these companies set out with awful intentions. But as Lanice Sims told us yesterday, your intentions ain&rsquo;t shit. at the end of the day, it&rsquo;s people in our industry &ndash; the people in this room &ndash; who are the ones who execute and implement these systems</p>
</aside>


            </section>
          <section>
              

<h1 id="a-problem-of-scale">a problem of scale</h1>

<aside class="notes"><p>the problem isn&rsquo;t just that these systems all have real-world consequences; software always has. we can go back to the 80&rsquo;s and look at the Therac-25 accidents, where software flaws in radiation machines killed 3 people. the problem is that those consequences are multiplied by the scale of these systems.</p>
</aside>


            </section>
          <section>
              

<h2 id="quantity-has-its-own-quality">quantity has its own quality</h2>

<aside class="notes"><p>a huge part of the value proposition of IoT and ML is the scale of the data involved: collecting massive amounts of data from edge computing devices, and processing massive amounts of data in ML models.</p>
</aside>


            </section>
          <section>
              

<h2 id="no-meaningfully-informed-consent">no meaningfully informed consent</h2>

<aside class="notes"><p>but the scope of machine learning and IoT is incomprehensible to ordinary users. if you can determine through a ML model of someone&rsquo;s purchases not just that they are pregnant but that they&rsquo;re in the 3rd trimester, this isn&rsquo;t a piece of data that the consumer willingly and knowingly shared with you, or have any way of knowing you could get. creating informed and meaningful consent is all but impossible</p>
</aside>


            </section>
          <section>
              

<p><img src="img/in_X_lines_01.png" alt="&quot;in X lines of Python&quot;" />
<span class='fragment '
  >
  <img src="img/in_X_lines_02.png" alt="&quot;in X lines of Python&quot;" />
</span>
<span class='fragment '
  >
  <img src="img/in_X_lines_03.png" alt="&quot;in X lines of Python&quot;" />
</span>
<span class='fragment '
  >
  <img src="img/in_X_lines_04.png" alt="&quot;in X lines of Python&quot;" />
</span>
<span class='fragment '
  >
  <img src="img/in_X_lines_05.png" alt="&quot;in X lines of Python&quot;" />
</span>
<span class='fragment '
  >
  <img src="img/in_X_lines_06.png" alt="&quot;in X lines of Python&quot;" />
</span>
<span class='fragment '
  >
  <img src="img/in_X_lines_07.png" alt="&quot;in X lines of Python&quot;" />
</span></p>

<aside class="notes"><p>what also makes ML especially problematic is that its inherent complexity is not being respected, only its results. &ldquo;easy machine learning in 100 lines of Python!&rdquo;. <em>many</em> users of ML treat the tools as magic and the models as binary blobs: a black box into which numbers go and decisions come out.</p>
</aside>


            </section>
          <section>
              

<p><img src="img/ugly-t-shirt.jpg" alt="picture of a ugly t-shirt that fools facial recognition" /></p>

<p><em><a href="https://www.theatlantic.com/technology/archive/2013/10/avoid-facial-detection-algorithms-with-a-t-shirt/280253/">Robison Meyer, The Atlantic, 3 Oct 2013</a></em></p>

<aside class="notes"><p>because ML&rsquo;s chaotic behavior is poorly understood, attacks on it can have open-ended results. maybe today someone is using a William Gibson&rsquo;s Ugly T-Shirt to protect their identity from ubiquitous law enforcement use of facial recognition&hellip;</p>
</aside>


            </section>
          <section>
              

<p><img src="img/face-recognition-glasses.png" alt="Verge: These glasses trick facial recognition software into thinking you're someone else" /></p>

<p><em><a href="https://www.theverge.com/2016/11/3/13507542/facial-recognition-glasses-trick-impersonate-fool">James Vincent, The Verge, 3 Nov 2016</a></em></p>

<aside class="notes"><p>&hellip; but could I give you an innocuous object that makes that system see you as a wanted terrorist to &ldquo;SWAT&rdquo; unsuspecting targets?</p>
</aside>


            </section>
          <section>
              

<p><img src="img/adversarial-toaster.png" alt="picture of an adversarial sticker that fools machine vision into thinking any object is a toaster" /></p>

<p><em><a href="https://www.theverge.com/2018/1/3/16844842/ai-computer-vision-trick-adversarial-patches-google">James Vincent, The Verge, 3 Jan 2018</a></em></p>

<aside class="notes"><p>what happens when a banana looks, not like a toaster, but a bomb or weapon? when an &ldquo;accident&rdquo; of that kind occurs, do the engineers of the system bear responsibility for failing to protect against this kind of &ldquo;side channel&rdquo; attack?</p>
</aside>


            </section>
          <section>
              

<p><img src="img/road-signs.png" alt="New Stack: Camouflaged Graffiti on Road Signs Can Fool Machine Learning Models" /></p>

<p><em><a href="https://thenewstack.io/camouflaged-graffiti-road-signs-can-fool-machine-learning-models/">Kimberly Mok, The New Stack, 14 Sep 2017</a></em></p>

<aside class="notes"><p>the flaws of self-driving vehicles and the organizations operating them seem terrifying enough without adding adversarial environments into the mix. this is unremarkable graffiti&hellip;</p>
</aside>


            </section>
          <section>
              

<blockquote>
<p>The team found that with this approach, they were able to confuse a machine 100 percent of the time into classifying a stop sign as a 45-mile-per-hour speed limit sign, and a right-turn sign as a stop sign.</p>
</blockquote>

<p><em><a href="https://thenewstack.io/camouflaged-graffiti-road-signs-can-fool-machine-learning-models/">Kimberly Mok, The New Stack, 14 Sep 2017</a></em></p>

<aside class="notes"><p>but it was used to confuse a machine vision algorithm <em>100%</em> of the time. these attacks are unreasonably effective!</p>
</aside>


            </section>
          <section>
              

<h2 id="embedded-industry-stuck-in-archaic-threat-model">embedded industry stuck in archaic threat model</h2>

<aside class="notes"><p>embedded industry stuck in archaic threat model. they&rsquo;re still shipping devices with shared private keys and hard-coded passwords. we used to say things like &ldquo;well if you have physical possession then it&rsquo;s game over&rdquo; but that&rsquo;s <em>always</em> the case with IoT devices. But we have answer to that: &ldquo;secure boot&rdquo; using TPM to sign the bootloader and OS updates. But this is treated as an expensive add-on rather than the default</p>
</aside>


            </section>
          <section>
              

<p><img src="img/hawkbit-logo.png" alt="Hawkbit logo" /></p>

<p><em><a href="http://www.eclipse.org/hawkbit/">http://www.eclipse.org/hawkbit/</a></em></p>

<aside class="notes"><p>existing solutions for IoT Over-the-Air updates (OTA) are mostly research projects at best (ex. Hawkbit, which depending on how you look at it is either an insecure-by-default toy, or an overcomplicated kit-of-parts)&hellip;</p>
</aside>


            </section>
          <section>
              

<p><img src="img/aws-iot.png" alt="diagram of AWS IoT products" /></p>

<p><em><a href="https://developer.amazon.com/de/blogs/post/Tx3828JHC7O9GZ9/Using-Alexa-Skills-Kit-and-AWS-IoT-to-Voice-Control-Connected-Devices">Robert McCauley, Alexa Blogs, 3 May 2016</a></em></p>

<aside class="notes"><p>&hellip; or require trust delegation to third parties like AWS IoT that throw-away the guarantees about provenance that put secure boot at risk</p>
</aside>


            </section>
          <section>
              

<h2 id="our-abdication-of-responsibility-invites-political-remedy">Our abdication of responsibility invites political remedy</h2>

<aside class="notes"><p>the problem with our complacency on this as an industry is that it invites someone to &ldquo;do something&rdquo; about it.</p>
</aside>


            </section>
          <section>
              

<p><img src="img/congress.jpg" alt="stock image photo of US Congress" /></p>

<p><em><a href="https://www.brookings.edu/wp-content/uploads/2016/06/congress006-1.jpg">https://www.brookings.edu/wp-content/uploads/2016/06/congress006-1.jpg</a></em></p>

<aside class="notes"><p>like these geniuses. and when they decide to act, the legal remedy is likely to be ham-fisted</p>
</aside>


            </section>
          <section>
              

<h2 id="we-have-to-do-something">&ldquo;We have to do something!&rdquo;</h2>

<h2 id="this-is-something">&ldquo;This is something&rdquo;</h2>

<h2 id="we-must-do-this">&ldquo;We must do this!&rdquo;</h2>

<aside class="notes"><p>we&rsquo;ve seen this over and over again. look at the evergreen fights over encryption, where the government thinks they can somehow make math available only to &ldquo;Good People&rdquo;</p>
</aside>


            </section>
          <section>
              

<p><img src="img/series-of-tubes.png" alt="caption of Senator Ted Stevens &quot;series of tubes&quot; remarks" /></p>

<p><em><a href="https://imgur.com/gallery/2mIObIu">https://imgur.com/gallery/2mIObIu</a></em></p>

<aside class="notes"><p>This is particularly a problem here in the US where virtually <em>all</em> our lawmakers at the federal level are either incompetent&hellip;</p>
</aside>


            </section>
          <section>
              

<p><img src="img/ajit-pai.png" alt="intentionally unflattering picture of FCC Chairman Ajit Pai" /></p>

<p><em><a href="https://www.engadget.com/2017/05/18/fcc-chairman-net-neutrality-1996/">Terrence O&rsquo;Brien, engadget, 18 May 2017</a></em></p>

<aside class="notes"><p>&hellip; or they (and their minions) are entirely co-opted by interests that are hostile to the greater public</p>
</aside>


            </section>
          <section>
              

<blockquote>
<p>Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an &ldquo;AS IS&rdquo; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License.</p>
</blockquote>

<p><em><a href="https://www.apache.org/licenses/LICENSE-2.0">https://www.apache.org/licenses/LICENSE-2.0</a></em></p>

<aside class="notes"><p>today much of our work is protected by saying &ldquo;hey we don&rsquo;t warranty this for fitness for any particular use.&rdquo; (this is the Apache license but other OSS licenses are similar, as are most EULAs.) these agreements aren&rsquo;t magic talismans; we&rsquo;ve been <em>allowed</em> to get away with this, but that could change.</p>
</aside>


            </section>
          <section>
              

<h2 id="do-you-want-to-be-personally-liable-for-bugs-in-your-code">Do you want to be personally liable for bugs in your code?</h2>

<aside class="notes"><p>anyone here want to carry malpractice insurance?</p>
</aside>


            </section>
          <section>
              

<p><img src="img/gdpr-block.png" alt="Apibility: GDPR for lazy people: block all European users with Cloudflare workers" /></p>

<p><em><a href="https://apility.io/2018/05/25/gdpr-lazy-block-european-users-cloudflare-workers/">https://apility.io/2018/05/25/gdpr-lazy-block-european-users-cloudflare-workers/</a></em></p>

<aside class="notes"><p>but even in the face of reasonable privacy law like the GDPR, we&rsquo;ve seen our industry act with utter lack of professionalism. After 2 <em>years</em> notice, this &ldquo;I&rsquo;m taking my ball home&rdquo; is the best that orgs can do?</p>
</aside>


            </section>
          <section>
              

<p><img src="img/gdpr-hackernews.png" alt="screenshot of childish HackerNews comment" /></p>

<p><em><a href="https://news.ycombinator.com/item?id=16954306">https://news.ycombinator.com/item?id=16954306</a></em></p>

<aside class="notes"><p>top post on The Orange Site: &ldquo;what many posters here miss is that a big group of tech people have no interest in dealing with legal matters.&rdquo; aw, poor baby! you don&rsquo;t get to be part of a world-impacting profession and pretend there are no real world consequences. childish!</p>
</aside>


            </section>
          <section>
              

<p><img src="img/gdpr-internet-of-shit.png" alt="@internetofshit on Twitter: &quot;Hi just letting you know you can't use your lights anymore because we're slathering your data around and GDPR is here&quot;" /></p>

<p><em><a href="https://twitter.com/internetofshit/status/999619364541394944">https://twitter.com/internetofshit/status/999619364541394944</a></em></p>

<aside class="notes"><p>on the other hand, if this is what organizations are doing&hellip;</p>
</aside>


            </section>
          <section>
              

<p><img src="img/gdpr-unrollme.png" alt="TechCrunch: Unroll.me to close to EU users saying it can't comply with GDPR" /></p>

<p><em><a href="https://techcrunch.com/2018/05/05/unroll-me-to-close-to-eu-users-saying-it-cant-comply-with-gdpr/">Natasha Lomas, Tech Crunch, 5 May 2018</a></em></p>

<aside class="notes"><p>&hellip;then maybe it&rsquo;s all working as intended</p>
</aside>


            </section>
          <section>
              

<h1 id="feature-not-bug">feature, not bug</h1>

<aside class="notes"><p>if you can&rsquo;t do your job to protect the privacy of users and have to close up shop: Mission. Accomplished.</p>
</aside>


            </section>
          <section>
              

<blockquote>
<p>Salesforce CEO Marc Benioff thinks America needs &ldquo;a national privacy law&hellip; that probably looks a lot like GDPR.</p>

<p>&ldquo;This is going to help our industry&hellip; It&rsquo;s going to set the guardrails around trust, around safety. It&rsquo;s going to provide the ability for the customers to interact with great next generation technologies in a safe way.&rdquo;</p>
</blockquote>

<p><em><a href="https://www.theregister.co.uk/2018/05/30/salesforce_q1_2019/?">Simon Sharwood, The Register, 30 May 2018</a></em></p>

<aside class="notes"><p>It doesn&rsquo;t have to be that way. In this interview with Marc Benioff he points out this can be good for our industry. It&rsquo;ll &ldquo;set the guardrails&rdquo;</p>
</aside>


            </section>
          <section>
              

<blockquote>
<p>Benioff went on to say that as artificial intelligence is used in customer service, &ldquo;that starts to cross the line on what is trust. And that&rsquo;s where our industry really has to come forward and say we&rsquo;re going to make sure that these technologies are trust-based. And I think the Europeans definitely got that figured out.&rdquo;</p>
</blockquote>

<p><em><a href="https://www.theregister.co.uk/2018/05/30/salesforce_q1_2019/?">Simon Sharwood, The Register, 30 May 2018</a></em></p>

<aside class="notes"><p>And this is deeply important because we as an industry have failed to set those guardrails for ourselves. And the larger society (our users) are starting to see this. It&rsquo;s not too late. What can we do?</p>
</aside>


            </section>
          <section>
              

<h1 id="consent">Consent</h1>

<aside class="notes"><p>[10:00]
consent is the <em>only</em> workable guiding model when we&rsquo;re talking about relationships between individual people. ex. I consent to being part of your research. You consent to engaging in conversation with me. When you decide you don&rsquo;t want to be in a relationship you can withdraw consent and your partner respects that. We <em>hopefully</em> all understand this by now?</p>
</aside>


            </section>
          <section>
              

<h1 id="individual-consent-vs-the-community">Individual Consent vs The Community</h1>

<aside class="notes"><p>But consent has some limits once we get a lot more people involved. Although individual consent is the basis of liberal democracy, there are some times when we decide that the will of the community overrides the consent of an individual.</p>
</aside>


            </section>
          <section>
              

<p><img src="img/vaccines.jpg" alt="cartoon of anti-vaxxer releasing a monster" /></p>

<p><em><a href="http://www.startribune.com/sack-cartoon-vaccinations/289998831/">Steve Sack, Star Tribune, 27 Jan 2015</a></em></p>

<aside class="notes"><p>We expect everyone to pay their taxes. We ask that people are vaccinated. And the boundaries of individual vs community consent vary by culture. ex. in the EU they protect individual consent strongly&hellip;</p>
</aside>


            </section>
          <section>
              

<p><img src="img/kelo-vs-new-london.png" alt="Washington Post: The story behind Kelo v City of New London" /></p>

<p><em><a href="https://www.washingtonpost.com/news/volokh-conspiracy/wp/2015/05/29/the-story-behind-the-kelo-case-how-an-obscure-takings-case-came-to-shock-the-conscience-of-the-nation/">Ilya Somin, Washington Post, 29 May 2015</a></em></p>

<aside class="notes"><p>whereas in the US we have a mixed bag where businesses (which are supposed to be individuals) are often given the backing of the community to override individual consent, without all the responsibility of consent.</p>
</aside>


            </section>
          <section>
              

<p><img src="img/tumblr-gdpr.png" alt="@Millstab on Twitter: &quot;Some seriously unethical UX design from @tumblr - forcing anyone to untick 350+ boxes in order to prevent EACH INDIVIDUAL AD COMPANY from using our data. Taking the piss.&quot;" /></p>

<p><em><a href="https://twitter.com/Millstab/status/999762424994594817">https://twitter.com/Millstab/status/999762424994594817</a></em></p>

<aside class="notes"><p>we talked earlier about how unexpectedly-derived data makes consent nearly impossible to understand. dark UX patterns make this worse by manipulating consent. &ldquo;you agreed to this!&rdquo;</p>
</aside>


            </section>
          <section>
              

<p><img src="img/externality-definition.png" alt="Google-supplied definition of externality" /></p>

<p><em><a href="https://www.google.com/search?q=externality">https://www.google.com/search?q=externality</a></em></p>

<aside class="notes"><p>we see flaws of consent at play when we look at the Uber accident. the woman who was killed didn&rsquo;t consent to be part of Uber&rsquo;s experimental driving program. she wasn&rsquo;t behind the wheel. what does &ldquo;consent&rdquo; mean when other human lives are treated as an externality? aside: what coal industry PR team infiltrated this into Google&rsquo;s results?</p>
</aside>


            </section>
          <section>
              

<h1 id="foundational-values">foundational values?</h1>

<aside class="notes"><p>the dynamic of community vs individual consent points to a problem defining foundational values (ref Bryan Cantrill&rsquo;s Monktober talk distinguishes between &ldquo;principles&rdquo; and &ldquo;values&rdquo;). if we can disagree on things like the balance of individual vs community (ex. EU vs US), how do we define shared values? we&rsquo;re not the first people to have this problem! so how do other professions solve this problem&hellip;?</p>
</aside>


            </section>
          <section>
              

<p><img src="img/sonia-1.png" alt="@soniagupta504 on Twitter: &quot;Lawyers and doctors enter their professions knowing, from the outset, just how heavy their burdens are. They hold human rights and life in their hands.&quot;" /></p>

<p><em><a href="https://twitter.com/soniagupta504/status/1011591288003575808">https://twitter.com/soniagupta504/status/1011591288003575808</a></em></p>

<aside class="notes"><p>Sonia Gupta (who was up here on stage last night): &ldquo;Lawyers and doctors enter their professions knowing, from the outset, just how heavy their burdens are. They hold human rights and life in their hands.&rdquo;</p>
</aside>


            </section>
          <section>
              

<p><img src="img/sonia-2.png" alt="@soniagupta504 on Twitter: &quot;highly impactful professions know that their limits must be more stringent&quot;" /></p>

<p><em><a href="https://twitter.com/soniagupta504/status/1011591288003575808">https://twitter.com/soniagupta504/status/1011591288003575808</a></em></p>

<aside class="notes"><p>highly impactful professions know that their limits must be more stringent</p>
</aside>


            </section>
          <section>
              

<h2 id="with-great-power-comes-great-responsibility">&ldquo;With great power comes great responsibility&rdquo;</h2>

<aside class="notes"><p>or as all the comic book nerds in the room know it: &ldquo;with great power comes great responsibility&rdquo;</p>
</aside>


            </section>
          <section>
              

<h1 id="professional-ethics">professional ethics</h1>

<aside class="notes"><p>this is the basis of professional ethics</p>
</aside>


            </section>
          <section>
              

<h1 id="licensing-as">licensing as</h1>

<h1 id="self-regulation">self-regulation</h1>

<aside class="notes"><p>historically licensing and professional organizations (ex. AMA, AIA, ASME, bar associations) have arisen from the professions themselves rather than being imposed clumsily from the outside. they&rsquo;ve asked for the protection of regulation. government licensing requirements are typically delegated to the professional organizations</p>
</aside>


            </section>
          <section>
              

<h1 id="licensing-requires-monopoly">licensing requires monopoly</h1>

<aside class="notes"><p>certifications do exist. we see lots and lots of useless rent-seeking certifications today already (CompTIA, Project Management Institute). but professional certification w/o the consequences of regulation is basically toothless.  so before we get the government involved, it&rsquo;s worth considering what the side-effects of regulation-supported monopoly would be.</p>
</aside>


            </section>
          <section>
              

<blockquote>
<p>We do not believe that it is merely a coincidence that the entry and standards of practice are most strictly regulated for physicians, dentists, and veterinarians&hellip; where the costs of receiving poor services could be high or sometimes even catastrophic.</p>
</blockquote>

<p><em><a href="http://www.nber.org/papers/w10467.pdf">ref http://www.nber.org/papers/w10467.pdf</a></em></p>

<aside class="notes"><p>study from the National Bureau of Economic Research shows that professional organizations have acted mostly as a counter to information asymmetry rather than creating monopoly power. consumers of our services generally don&rsquo;t understand what they&rsquo;re buying, and so professional bodies provide an answer to those questions. I&rsquo;m not a doctor but I rely on the boards to tell me my doctor has met minimum standards of fair dealing, competence, and safety</p>
</aside>


            </section>
          <section>
              

<h1 id="gatekeeping">gatekeeping</h1>

<aside class="notes"><p>&hellip; so in other words they serve as gatekeepers. well, that&rsquo;s a little problematic isn&rsquo;t it? we have enough barriers to entry in our profession as it is. we still have a <em>lot</em> of work to do. so adding new barriers like &ldquo;you must have <em>this</em> degree&rdquo; or &ldquo;you must have this many years experience working under a licensed developer&rdquo; seem like they&rsquo;d work against the direction we want to take our industry, right?</p>
</aside>


            </section>
          <section>
              

<h1 id="shared-ethical-baseline">shared ethical baseline</h1>

<aside class="notes"><p>but an advantage of a professional organization&rsquo;s ethical framework is that it at least gives a shared baseline: right now we&rsquo;re all trying to figure this out for ourselves. and we should hold each other to task</p>
</aside>


            </section>
          <section>
              

<h1 id="what-is-to-be-done">what is to be done?</h1>

<aside class="notes"><p>[15:00]
waiting for regulation seems like a bad idea. relying on consent hasn&rsquo;t really worked out. professional organizations seem fraught. but waiting for &ldquo;the industry&rdquo; to fix it isn&rsquo;t working. we all, individually, are the industry. need to take individual action.</p>


            </section>
          <section>
              

<p><img src="img/coed-ethics.png" alt="@containersoluti on Twitter: Very excited to announce that our conference on tech ethics @coedethics is sold out!" /></p>

<p><em><a href="https://twitter.com/containersoluti/status/1016624243868667904">https://twitter.com/containersoluti/status/1016624243868667904</a></em></p>

<aside class="notes"><p>we can talk about this stuff. COED:Ethics in London happening today, sponsored by the great folks at ContainerSolutions like Anne Currie and Jamie Dobson</p>
</aside>


            </section>
          <section>
              

<p><img src="img/techworkerscoalition.png" alt="Tech Workers Coalition: Worker Power in the Tech Industry" /></p>

<p><em><a href="https://techworkerscoalition.org/">https://techworkerscoalition.org/</a></em></p>

<aside class="notes"><p>we can influence the community. there are activist organizations that have sprung up in the last few years looking to push for broader change across many organizations, like the Tech Workers Coalition</p>
</aside>


            </section>
          <section>
              

<p><img src="img/microsoft-employees-ice.png" alt="arstechnica: Microsoft staff call on company to end ICE contract" /></p>

<p><em><a href="https://arstechnica.com/tech-policy/2018/06/microsoft-staff-call-on-company-to-end-ice-contract/">Peter Bright, Ars Technica, 20 Jun 2018</a></em></p>

<aside class="notes"><p>we can influence our organizations. we see Microsoft employees pushing for change in what customers their organization serves</p>
</aside>


            </section>
          <section>
              

<p><img src="img/google-employees-military.png" alt="Reuters: Google to scrub US military deal protested by employees" /></p>

<p><em><a href="https://www.reuters.com/article/uk-alphabet-defense/google-to-scrub-u-s-military-deal-protested-by-employees-source-idUKKCN1IX5YC">Paresh Dave and Heather Somerville, Reuters, 1 Jun 2018</a></em></p>

<aside class="notes"><p>we&rsquo;ve seen successful protest by Googlers motivated by their conscience</p>
</aside>


            </section>
          <section>
              

<p><img src="img/amazon-employees-law-enforcement.png" alt="Gizmodo: Amazon Workers Demand Jeff Bezos Cancel Face Recognition Contracts With Law Enforcement" /></p>

<p><em><a href="https://gizmodo.com/amazon-workers-demand-jeff-bezos-cancel-face-recognitio-1827037509">Kate Conger, Gizmodo, 21 Jun 2018</a></em></p>

<aside class="notes"><p>and we&rsquo;ve seen Amazon employees pushing for change</p>
</aside>


            </section>
          <section>
              

<h1 id="what-about">what about&hellip;?</h1>

<aside class="notes"><p>&ldquo;whataboutism&rdquo;: because we have a restricted set of common values, it isn&rsquo;t constructive to see e.g. Microsoftees protesting ICE but then turn around and say &ldquo;what about their military contracts?&rdquo; or &ldquo;what about that time when MSFT&rsquo;s CEO from 17 years ago said meanie-head things about open source?&rdquo; This is not helpful. At least they&rsquo;re doing something instead of snarking about it on twitter</p>
</aside>


            </section>
          <section>
              

<h1 id="who-s-hiring">Who&rsquo;s hiring?</h1>

<aside class="notes"><p>the labor market for our profession gives us enormous power right now. we can push hard for better hiring policies. we can push hard for D&amp;I efforts. we can push hard for our organizations to be better</p>
</aside>


            </section>
          <section>
              

<h1 id="ensure-your-own-mask-is-secure">ensure your own mask is secure</h1>

<aside class="notes"><p>that being said, we should also cut each other some slack. you don&rsquo;t know much about the circumstances of any particular person (do they have health problems? family to take care of?). so while we should be holding each other to task, individuals need to make their own decisions about where they work and that doesn&rsquo;t make them The Enemy. (unless they work at Palantir, just sayin&rsquo;)</p>
</aside>


            </section>
          <section>
              

<h1 id="direct-action">direct action</h1>

<aside class="notes"><p>what is available to everyone regardless of our work conditions, and perhaps more effective than anything else we can do, is direct action at an engineering level. what choices do we make as technologists?</p>
</aside>


            </section>
          <section>
              

<h2 id="immutable">Immutable?</h2>

<p><img src="img/Kafka.png" alt="diagram of Apache Kafka commit log and typical consumers" /></p>

<p><a href="https://vision.cloudera.com/apache-kafka-a-platform-for-real-time-data-streams-part-1/">Jay Kreps, Cloudera, 11 June 2015</a></p>

<aside class="notes"><p>the pipelines we use to ingest data in many orgs are using immutable event stores. this means that once data is written it&rsquo;s never really erased. if you&rsquo;re keeping the entire commit history then you&rsquo;re going to have a lot of problems with GDPR. (did you remember your backups?)</p>
</aside>


            </section>
          <section>
              

<h2 id="immutable-1">&ldquo;Immutable&rdquo;</h2>

<p><img src="img/Kafka-per-user.png" alt="diagram of Apache Kafka commit log and typical consumers, with per-user encryption keys" /></p>

<aside class="notes"><p>but with changes at the application level, we can make this work. for example, we can use per-user encryption keys for a stream. this way we have encryption at rest, which is great, but we&rsquo;ve also made our immutable event infrastructure compatible with the &ldquo;right to be forgotten&rdquo;: you delete the per-user key from the key store and the data is forever unreadable.</p>
</aside>


            </section>
          <section>
              

<pre><code class="language-python">def should_brake(road):
    if road.contains(object.HUMAN):
        log.warn(&quot;oh shit!&quot;)
        # TODO: this is causing erratic driving on false
        # positive detection. Uncomment this once we have
        # that solved. Someone should remind the field
        # engineers to tell the test drivers they need to
        # pay attention to the road.
        # return True
    return False
</code></pre>

<aside class="notes"><p>we need to take responsibility for quality proportional to the risks involved with the software. certainly this is obvious in directly-life-impacting software like self-driving cars, but it extends to every piece of software with real world impact. we need to be part of the chain of safety around these systems.</p>
</aside>


            </section>
          <section>
              

<h1 id="secure-by-design">secure by design</h1>

<aside class="notes"><p>we need to take responsibility for having deep understanding of how the security of our systems are bootstrapped. and we need to make decisions about that aligned with our values.</p>
</aside>


            </section>
          <section>
              

<p><img src="img/secureboot.jpg" alt="diagram showing secure boot stages" /></p>

<p><em><a href="https://www.iconlabs.com/prod/products/device-protection/floodgate-secure-boot">https://www.iconlabs.com/prod/products/device-protection/floodgate-secure-boot</a></em></p>

<aside class="notes"><p>in the case of IoT one of the options is secure boot.  you&rsquo;ll provide the factory where the product is assembled with the public keys you want to have loaded into your chip&rsquo;s trusted platform module. This makes it so that only <em>your</em> signed bootloader and OS can be loaded onto the device</p>
</aside>


            </section>
          <section>
              

<p><img src="img/vault.png" alt="Hashicorp Vault logo" /></p>

<aside class="notes"><p>this means ensuring that your organization has PKI infrastructure in place to sign the bootloader and OS. getting familiar with secret stores like Hashicorp Vault to build the foundation of this infrastructure is a good practice</p>
</aside>


            </section>
          <section>
              

<p><img src="img/john_deere.png" alt="Motherboard: Why American Farmers Are Hacking Their Tractors With Ukranian Firmware" /></p>

<p><em><a href="https://motherboard.vice.com/en_us/article/xykkkd/why-american-farmers-are-hacking-their-tractors-with-ukrainian-firmware">Jason Koebler, Motherboard, 21 Mar 2017</a></em></p>

<aside class="notes"><p>but there&rsquo;s a tradeoff here. if you have secure boot that means your users will find it that much more difficult (if not impossible) to modify and repair the device firmware. this is a business decision <em>and</em> a decision about values; maybe the answer is different if the end user is a consumer vs a business? not making a decision one way or another means abdicating your responsibility as an engineer</p>
</aside>


            </section>
          <section>
              

<h1 id="secure-communication">secure communication</h1>

<aside class="notes"><p>when we deploy IoT devices we need to ensure we&rsquo;re taking advantage of modern TLS options. you wouldn&rsquo;t transmit the login form for your web application over plain text, right? (right?!)</p>
</aside>


            </section>
          <section>
              

<p><img src="img/cloudflare_tls_1-3.png" alt="CloudFlare blog: TLS1.3 is going to save us all, and other reasons why IoT is still insecure" /></p>

<aside class="notes"><p>IoT devices typically have limited compute power, so folks avoid encryption. but we can solve this with modern choices: TLS1.3 reduces round-trips and elliptic curve keys use less memory than RSA keys. likewise, although you can cram MQTT into TLS, modern protocols like CoAP include mutually authenticated TLS. TLS isn&rsquo;t &ldquo;end-user facing&rdquo; in IoT (unlike a browser), but secure communication isn&rsquo;t about user optics but actually protecting the user.</p>
</aside>


            </section>
          <section>
              

<h1 id="ml-models">ML models</h1>

<aside class="notes"><p>what about our engineering decisions in ML?</p>
</aside>


            </section>
          <section>
              

<p><img src="img/neural_network.png" alt="diagram of a simple neural network" /></p>

<p><a href="https://commons.wikimedia.org/wiki/File:Neural_network_bottleneck_achitecture.svg">https://commons.wikimedia.org/wiki/File:Neural_network_bottleneck_achitecture.svg</a></p>

<aside class="notes"><p>let&rsquo;s look at one ML process called Neural Net (or Convolutional NN). each of the nodes in this diagram is just a function that takes a vector (an array) and runs a filter function over it (typically the filter itself will take a subset of the vector at a time).</p>
</aside>


            </section>
          <section>
              

<p><img src="img/iris_dataset.png" alt="scatterplot of the classic public Iris dataset" /></p>

<p><a href="https://commons.wikimedia.org/wiki/File:Iris_dataset_scatterplot.svg">https://commons.wikimedia.org/wiki/File:Iris_dataset_scatterplot.svg</a></p>

<aside class="notes"><p>iterating on the weights of those inputs allows us to extract &ldquo;features&rdquo; from the training inputs that result in &ldquo;interesting&rdquo; classifications (decisions). this is the classic iris dataset</p>
</aside>


            </section>
          <section>
              

<p><img src="img/deep_neural_network.png" alt="diagram of a deep neural net with multiple stages shown" /></p>

<p><a href="https://cdn.edureka.co/blog/wp-content/uploads/2017/05/Deep-Neural-Network-What-is-Deep-Learning-Edureka.png">https://cdn.edureka.co/blog/wp-content/uploads/2017/05/Deep-Neural-Network-What-is-Deep-Learning-Edureka.png</a></p>

<aside class="notes"><p>each layer builds on the layers that came before it. but these classifications are just regions in mathematical space here</p>
</aside>


            </section>
          <section>
              

<pre><code class="language-python">def save(model, filename):
    pickle.dump(model, open(filename, 'wb'))
</code></pre>

<aside class="notes"><p>and then when we&rsquo;re done training, we take the result of all those weights and call it our model and just serialize the whole damn thing and ship it to prod. we can pass real world data in through the same model and out comes the classifications.</p>
</aside>


            </section>
          <section>
              

<h1 id="ml-models-are-state">ML models are state</h1>

<aside class="notes"><p>our entire industry has unified around our worries about statefulness. &ldquo;run your applications as stateless containers! with k8s! let your cloud provider lock-in &ndash; I mean securely host &ndash; all your stateful applications!&rdquo; But ML is the ultimate stateful application. you&rsquo;re using software to generate these software models and you ship those opaque blobs. let&rsquo;s get the Functional Programming crowd in a twist: the entire application is a side-effect! how can we influence those side-effects?</p>
</aside>


            </section>
          <section>
              

<p><img src="img/machine-learning-tech-debt.png" alt="Google AI whitepaper: Machine Learning, the High Interest Credit Card of Technical Debt" /></p>

<p><em><a href="https://ai.google/research/pubs/pub43146">https://ai.google/research/pubs/pub43146</a></em></p>

<aside class="notes"><p>we desperately need better tooling to design-out unexpected behaviors in ML. We have undeclared consumers/dependencies and hidden &ldquo;strange loops&rdquo; of feedback. These represent a source of technical debt. and it&rsquo;s the worst kind of technical debt &ndash; it&rsquo;s what John Allspaw and the SNAFUcatchers call &ldquo;shadow debt&rdquo; that&rsquo;s taken on unknowingly.</p>
</aside>


            </section>
          <section>
              

<blockquote>
<p>To make great products:</p>

<p>do machine learning like the great engineer you are, not like the great machine learning expert you aren&rsquo;t.</p>

<p>&hellip;</p>

<p>Insofar as well-being and company health is concerned, human judgement is required to connect any machine learned objective to the nature of the product you are selling and your business plan.</p>
</blockquote>

<p><em><a href="https://developers.google.com/machine-learning/rules-of-ml/">Martin Zinkevich (Google), Rules of Machine Learning: Best Practices for ML Engineering</a></em></p>

<aside class="notes"><p>we need to remember with any ML project that human judgement is required to connect the algorithms to their impact with the real world &ndash; our business, our industry, and the community as a whole</p>
</aside>


            </section>
          <section>
              

<h1 id="sql-ml">SQL &gt; ML</h1>

<aside class="notes"><p>first rule should always be: why are we choosing ML over some well-tuned SQL or other algorithm? why are we choosing to use a method with chaotic feedback mechanisms instead of something that&rsquo;s simple and deterministic? imagine explaining to your CTO why you chose a web framework that allowed for undefined behavior on hostile inputs! is this simply because of resume-driven development?</p>
</aside>


            </section>
          <section>
              

<h2 id="models-should-be-testable-and-human-interpretable">models should be testable and human-interpretable</h2>

<aside class="notes"><p>when we do choose ML, we should choose our approach with interpretability in mind. simple linear or logarithmic regression models are easier to debug and avoid unexpected feedback loops than models that self-optimize. remove unused features. quantify any unexpected behavior and build tests for it. these behaviors are both technical debt and avenues for adversarial input</p>
</aside>


            </section>
          <section>
              

<h1 id="align-training-data-with-real-world-demographics">align training data with real world demographics</h1>

<aside class="notes"><p>choose ML training inputs that reflect the population. this is a fortunate case where we can easily align engineering ethics with business needs in a way the business understands (ex. &ldquo;if we pick machine vision training data that reflects real demographics, we can avoid the embarrassment for our organization of having to explain why our software acts racist.&rdquo;) win-win, eh?</p>
</aside>


            </section>
          <section>
              

<h1 id="technical-leadership">technical leadership</h1>

<aside class="notes"><p>we can&rsquo;t rely on project managers or business analysts to take the lead on designing our systems ethically because they simply may not understand the side-effects. you&rsquo;re the technical professional: they&rsquo;re expecting you to take the lead on this!</p>
</aside>


            </section>
          <section>
              

<h1 id="best-practice-is">&ldquo;best practice is&hellip;&rdquo;</h1>

<aside class="notes"><p>&ldquo;the best thing about best practices is there are so many of them to choose from.&rdquo; you don&rsquo;t need to ask permission from your business analysts and project managers on opinions that are purely technical</p>
</aside>


            </section>
          <section>
              

<p><img src="img/jira-clock-puncher.png" alt="cartoon punching a clock with a JIRA ticket" />
<span class='fragment '
  >
  <img src="img/jira-clock-puncher.png" alt="cartoon punching a clock with a JIRA ticket" />
</span>
<span class='fragment '
  >
  <img src="img/jira-clock-puncher.png" alt="cartoon punching a clock with a JIRA ticket" />
</span>
<span class='fragment '
  >
  <img src="img/jira-clock-puncher.png" alt="cartoon punching a clock with a JIRA ticket" />
</span>
<span class='fragment '
  >
  <img src="img/jira-clock-puncher.png" alt="cartoon punching a clock with a JIRA ticket" />
</span>
<span class='fragment '
  >
  <img src="img/jira-clock-puncher.png" alt="cartoon punching a clock with a JIRA ticket" />
</span>
<span class='fragment '
  >
  <img src="img/jira-clock-puncher.png" alt="cartoon punching a clock with a JIRA ticket" />
</span>
<span class='fragment '
  >
  <img src="img/jira-clock-puncher.png" alt="cartoon punching a clock with a JIRA ticket" />
</span>
<span class='fragment '
  >
  <img src="img/jira-clock-puncher.png" alt="cartoon punching a clock with a JIRA ticket" />
</span>
<span class='fragment '
  >
  <img src="img/jira-clock-puncher.png" alt="cartoon punching a clock with a JIRA ticket" />
</span>
<span class='fragment '
  >
  <img src="img/jira-clock-puncher.png" alt="cartoon punching a clock with a JIRA ticket" />
</span>
<span class='fragment '
  >
  <img src="img/jira-clock-puncher.png" alt="cartoon punching a clock with a JIRA ticket" />
</span>
<span class='fragment '
  >
  <img src="img/jira-clock-puncher.png" alt="cartoon punching a clock with a JIRA ticket" />
</span>
<span class='fragment '
  >
  <img src="img/jira-clock-puncher.png" alt="cartoon punching a clock with a JIRA ticket" />
</span>
<span class='fragment '
  >
  <img src="img/jira-clock-puncher.png" alt="cartoon punching a clock with a JIRA ticket" />
</span></p>

<aside class="notes"><p>formal Agile methodologies contribute to the problem. &ldquo;act ethically&rdquo; doesn&rsquo;t have any story points in this epic. But you&rsquo;re not a assembly line worker punching out JIRA tickets, you&rsquo;re a professional engineer! act like it!</p>
</aside>


            </section>
          <section>
              

<blockquote>
<p>We aren&rsquo;t a craft anymore. We might feel like artisans with laptops but what we produce could potentially be in front of a significant chunk of the human race by lunchtime. We&rsquo;re not hand-crafting dovetail joints here.</p>
</blockquote>

<p><em><a href="https://www.theregister.co.uk/2018/03/01/ethics_yeah_thats_great_but_do_they_scale/">Anne Currie, The Register, 1 Mar 2018</a></em></p>

<aside class="notes"><p>leave you with this from Anne Currie at ContainerSolutions. &ldquo;We aren&rsquo;t a craft anymore. We might feel like artisans with laptops but what we produce could potentially be in front of a significant chunk of humanity by lunchtime.&rdquo;</p>
</aside>


            </section>
          <section>
              

<h1 id="let-s-get-to-work">let&rsquo;s get to work</h1>

<aside class="notes"><p>what we do has impact. let&rsquo;s get to work. thanks, folks!</p>
</aside>
</aside>

            </section>
          
  </div>
</div>


    <script type="application/json" id="reveal-hugo-page-params">{"custom_theme":"./css/devopsdays.css","highlight_theme":"solarized-dark"}</script>
    <script type="application/json" id="reveal-hugo-site-params">{"theme":"beige"}</script>
    <script type="application/json" id="reveal-hugo-defaults">{"center":true,"controls":true,"highlight_theme":"default","history":true,"progress":true,"theme":"black","transition":"slide"}</script>
    <script type="text/javascript">
      window.revealHugoDependencies = {
        dependencies: [
          { src: './lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: './plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
          { src: './plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
          { src: './plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
          { src: './plugin/zoom-js/zoom.js', async: true },
          
          { src: 'reveal-hugo\/plugin\/notes\/notes.js', async: true }
      ]};
    </script>
    <script src="./lib/js/head.min.js"></script>
    <script src="./js/reveal.js"></script>
    <script type="text/javascript">
      
      
      function camelize(map) {
        if (map) {
          Object.keys(map).forEach(function(k) {
            newK = k.replace(/(\_\w)/g, function(m) { return m[1].toUpperCase() });
            if (newK != k) {
              map[newK] = map[k];
              delete map[k];
            }
          });
        }
        return map;
      }
      
      var revealHugoPageParams = JSON.parse(document.getElementById('reveal-hugo-page-params').innerHTML);
      var revealHugoSiteParams = JSON.parse(document.getElementById('reveal-hugo-site-params').innerHTML);
      var revealHugoDefaults = JSON.parse(document.getElementById('reveal-hugo-defaults').innerHTML);

      
      var options = Object.assign(
        camelize(revealHugoDefaults),
        camelize(revealHugoSiteParams),
        camelize(revealHugoPageParams),
        revealHugoDependencies);
      Reveal.initialize(options);

    </script>
    
  </body>
</html>
