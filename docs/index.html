<!doctype html><html lang="en">
  <head>
	<meta name="generator" content="Hugo 0.42.2" />
    <meta charset="utf-8">
    <title>DevOps Days MSP 2018</title>
    <meta name="description" content="">
    <meta name="author" content="">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <link rel="stylesheet" href="./css/reveal.min.css">
    
    
    
    
    <link rel="stylesheet" href="./css/devopsdays.css" id="theme">
    
    
    
    <link rel="stylesheet" href="./styles/solarized-dark.min.css">
    
    <script>
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = window.location.search.match( /print-pdf/gi ) ? './css/print/pdf.css' : './css/print/paper.css';
      document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>
    
    
  </head>
  <body>
    
  <div class="reveal">
  <div class="footer"><a href="https://twitter.com/0x74696d">@0x74696d</a></div>

  
  <div class="slides">
    
    <section>
              

<h1 id="they-didn-t-stop-to-think-if-they-should">They Didn&rsquo;t Stop To Think If They Should</h1>

<h4 id="tim-gross-machinist-labs">Tim Gross | Machinist Labs</h4>

<aside class="notes"><p>My name is Tim Gross from Machinist Labs, and my talk is titled &ldquo;They Didn&rsquo;t Stop to Think If They Should&rdquo;</p>
</aside>


            </section>
          <section>
              

<h2 id="or">or&hellip;</h2>

<h2 id="machine-learning-and-the-internet-of-unpatched-things">Machine Learning And The Internet Of Unpatched Things</h2>

<aside class="notes"><p>an alternate title could be&hellip;</p>
</aside>


            </section>
          <section>
              

<h2 id="or-1">or&hellip;</h2>

<h2 id="because-eternal-vigilance-is-the-price-of-liberty-we-have-to-talk-about-ethics-of-the-tech-industry-again">Because Eternal Vigilance is the Price of Liberty, We Have to Talk About Ethics of the Tech Industry Again</h2>


            </section>
          <section>
              

<blockquote>
<p>We&rsquo;ve updated our privacy policy!</p>

<p>The opinions expressed in this talk are the speaker&rsquo;s alone and do not reflect the view of this conference, your employer, or my mom. Any reference to or citation of any person or organization does not constitute or imply an endorsement or recommendation of the content of this talk. The speaker is grossly unqualified to tell you how to live you life. Your mileage may vary. Not to be used in the manufacture of nuclear weapons. By attending this talk the speaker hereby grants you an irrevocable, perpetual, non-exlusive, tranferable, worldwide license to be excellent to each other.</p>
</blockquote>

<aside class="notes"><p>why is it especially important to talk about ethics when we start talking about ML and IoT? What makes these things special? What makes them different from other software? Why do we as engineers even need to discuss this stuff? &ldquo;Ethics is hard!&rdquo;</p>
</aside>


            </section>
          <section>
              

<p><img src="img/uber-accident.png" alt="uber-self-driving" /></p>

<p><em><a href="https://www.washingtonpost.com/news/dr-gridlock/wp/2018/05/24/ntsb-self-driving-uber-did-not-have-emergency-braking-turned-on/">Michael Laris, Washington Post, 24 May 2018</a></em></p>

<aside class="notes"><p>Back in March a self-driving car operated by Uber killed a pedestrian. The NTSB investigation is still ongoing, but what is clear is that the vehicle had no business being operated without direct supervision. The sensing system had lots of false positives that caused it to brake erratically. So they turned the braking system off. But apparently that message never got to the folks who&rsquo;d turned off the vehicles&rsquo; own automatic braking nor to the attendee of the vehicle. So the vehicle &ldquo;saw&rdquo; the pedestrian and did nothing.</p>
</aside>


            </section>
          <section>
              

<p><img src="img/microsoft-chatbot.png" alt="microsoft-chatbot" /></p>

<p><em><a href="https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist">James Vincent, The Verge, 24 Mar 2016</a></em></p>

<aside class="notes"><p>A couple years ago, MSFT demonstrated their ML capabilities with the Tay chatbot. They let it loose to learn from whatever racist trolls wanted to teach it. The experiment was stopped and the whole debacle embarassed MSFT. Now clearly the horrible Internet people are primarily to blame here, but the researchers failed to anticipate the &ldquo;side channel&rdquo; attacks inherent to their system</p>
</aside>


            </section>
          <section>
              

<p><img src="img/uk-ransomware.png" alt="uk-ransomware" /></p>

<p><em><a href="https://www.theverge.com/2017/5/12/15630354/nhs-hospitals-ransomware-hack-wannacry-bitcoin">Russell Brandom, The Verge, 12 May 2017</a></em></p>

<aside class="notes"><p>in an IoT example, last year countless devices in UK hospitals were taken over by Wannacry ransomware. patient lives were put at risk not just because of poor patch hygine but because of the design flaws that allowed access to the devices in the first place and because of warped incentives in the government programs that produced the stolen malware in the first place.</p>
</aside>


            </section>
          <section>
              

<p><img src="img/tesla-autopilot.png" alt="tesla-autopilot" /></p>

<p><em><a href="https://www.theguardian.com/technology/2018/mar/31/tesla-car-crash-autopilot-mountain-view">The Guardian, 31 Mar 2018</a></em></p>

<aside class="notes"><p>Tesla&rsquo;s autopilot has been implicated in the deaths of several drivers at this point. We (and Tesla stockholders, apparently) are constantly reassured that these are the result of improper handling and not rushing these systems into the real world before they&rsquo;re ready.</p>
</aside>


            </section>
          <section>
              

<p><img src="img/target-pregnant.png" alt="target-pregnant" /></p>

<p><em><a href="https://www.forbes.com/sites/kashmirhill/2012/02/16/how-target-figured-out-a-teen-girl-was-pregnant-before-her-father-did/">Kashmir Hill, Forbes, 16 Feb 2012</a></em></p>

<aside class="notes"><p>Machine learning algorithms may know more about us and our loved ones than we do ourselves. Target was able to determine from customer purchases not just when customers are pregnant but at what stage of their pregnancy they were (buying unscented products during late stages, for example).</p>
</aside>


            </section>
          <section>
              

<p><img src="img/google-racist-autotag.png" alt="google-racist-autotag" /></p>

<p><em><a href="https://www.theguardian.com/technology/2015/jul/01/google-sorry-racist-auto-tag-photo-app">Jana Kasperkevic, The Guardian, 1 Jul 2015</a></em></p>

<aside class="notes"><p>ML is simultaneously unconsciously encoding our biases. (This one was so bad that I cropped it out of this picture.)</p>
</aside>


            </section>
          <section>
              

<p><img src="img/defeat-the-baby.png" alt="defeat-the-baby" />
  *
<em><a href="https://twitter.com/yipe/status/1005555741153902592">https://twitter.com/yipe/status/1005555741153902592</a></em></p>

<aside class="notes"><p>we&rsquo;ve invited these systems into our homes&hellip; (and ok, yeah this is funny)</p>
</aside>


            </section>
          <section>
              

<p><img src="img/alexa-eavesdropping.png" alt="alexa-eavesdropping" /></p>

<p><em><a href="https://www.huffingtonpost.com/entry/alexa-eavesdropping-portland-familiy_us_5b0727cae4b0fdb2aa51b23e">Laura Stevens, Huffington Post, 24 May 2018</a></em></p>

<aside class="notes"><p>&hellip; with almost no oversight into what they&rsquo;re doing. We have unaccountable closed source microphones in our homes, who do who-knows-what with that information</p>
</aside>


            </section>
          <section>
              

<p><img src="img/marco-rogers.png" alt="marco" /></p>

<aside class="notes"><p>what all these stories have in common is that like any other failure there&rsquo;s almost certainly no &ldquo;root cause&rdquo;; it&rsquo;s unlikely that anyone at these companies set out with awful intentions. It&rsquo;s a complex combination of socio-technical systems and the incentives they set up. But at the end of the day, it&rsquo;s people in our industry &ndash; the people in this room &ndash; who are the ones who execute and implement these systems</p>
</aside>


            </section>
          <section>
              

<h1 id="a-problem-of-scale">a problem of scale</h1>

<aside class="notes"><p>the problem we have here is not just that these systems all have serious real-world consequences; we can look at the Therac-25 accidents in the 80&rsquo;s for examples of that. the problem is that those consequences are multiplied by the scale of these systems.</p>
</aside>


            </section>
          <section>
              

<h2 id="quantity-has-its-own-quality">quantity has its own quality</h2>

<aside class="notes"><p>a huge part of the value proposition of IoT and ML is the scale of the data involved: collecting massive amounts of data from edge computing devices, and processing massive amounts of data in ML models.</p>
</aside>


            </section>
          <section>
              

<h2 id="no-meaningfully-informed-consent">no meaningfully informed consent</h2>

<aside class="notes"><p>but the scope of machine learning and IoT is incomprensible to ordinary users. when you can determine through a ML model of someone&rsquo;s purchases not just that they are pregnant but that they&rsquo;re in the 3rd trimester, this isn&rsquo;t a piece of data that the consumer willingly and knowingly shared with you. creating informed and meaningful consent is all but impossible</p>
</aside>


            </section>
          <section>
              

<p><img src="img/in_X_lines_01.png" alt="in_X_lines" />
<span class='fragment '
  >
  <img src="img/in_X_lines_02.png" alt="in_X_lines" />
</span>
<span class='fragment '
  >
  <img src="img/in_X_lines_03.png" alt="in_X_lines" />
</span>
<span class='fragment '
  >
  <img src="img/in_X_lines_04.png" alt="in_X_lines" />
</span>
<span class='fragment '
  >
  <img src="img/in_X_lines_05.png" alt="in_X_lines" />
</span>
<span class='fragment '
  >
  <img src="img/in_X_lines_06.png" alt="in_X_lines" />
</span>
<span class='fragment '
  >
  <img src="img/in_X_lines_07.png" alt="in_X_lines" />
</span></p>

<aside class="notes"><p>what also makes ML and IoT especially problematic is that their inherent complexity is not being respected, only their results. amateurish engineering: &ldquo;easy machine learning in 100 lines of Python!&rdquo;. maybe ML tools are reasonably well-understood by the elite developers of the best organizations, but <em>many</em> users of ML treat the tools as magic and the models as binary blobs: a black box into which numbers go and decisions come out.</p>
</aside>


            </section>
          <section>
              

<p><img src="img/ugly-t-shirt.jpg" alt="ugly-t-shirt" /></p>

<p><em><a href="https://www.theatlantic.com/technology/archive/2013/10/avoid-facial-detection-algorithms-with-a-t-shirt/280253/">Robison Meyer, The Atlantic, 3 Oct 2013</a></em></p>

<aside class="notes"><p>because ML is poorly understood, attacks on it can have open-ended results. maybe today someone is using a William Gibson&rsquo;s Ugly T-Shirt to protect their identity from ubiquitous law enforcement use of facial recognition&hellip;</p>
</aside>


            </section>
          <section>
              

<p><img src="img/face-recognition-glasses.png" alt="face-recognition-glasses" /></p>

<p><em><a href="https://www.theverge.com/2016/11/3/13507542/facial-recognition-glasses-trick-impersonate-fool">James Vincent, The Verge, 3 Nov 2016</a></em></p>

<aside class="notes"><p>&hellip; but it doesn&rsquo;t take much imagination to see someone using this same system to &ldquo;SWAT&rdquo; unsuspecting targets</p>
</aside>


            </section>
          <section>
              

<p><img src="img/adversarial-toaster.png" alt="adversarial-toaster" /></p>

<p><em><a href="https://www.theverge.com/2018/1/3/16844842/ai-computer-vision-trick-adversarial-patches-google">James Vincent, The Verge, 3 Jan 2018</a></em></p>

<aside class="notes"><p>what happens when a banana looks, not like a toaster, but a bomb or weapon? when an &ldquo;accident&rdquo; of that kind occurs, do the engineers of the system bear responsibility for failing to protect against this kind of &ldquo;side channel&rdquo; attack?</p>
</aside>


            </section>
          <section>
              

<p><img src="img/road-signs.png" alt="road-signs" /></p>

<p><em><a href="https://thenewstack.io/camouflaged-graffiti-road-signs-can-fool-machine-learning-models/">Kimberly Mok, The New Stack, 14 Sep 2017</a></em></p>

<aside class="notes"><p>the flaws of self-driving vehicles and the organizations operating them seem terrifying enough without adding adversarial environments into the mix</p>
</aside>


            </section>
          <section>
              

<blockquote>
<p>The team found that with this approach, they were able to confuse a machine 100 percent of the time into classifying a stop sign as a 45-mile-per-hour speed limit sign, and a right-turn sign as a stop sign.</p>
</blockquote>

<p><em><a href="https://thenewstack.io/camouflaged-graffiti-road-signs-can-fool-machine-learning-models/">Kimberly Mok, The New Stack, 14 Sep 2017</a></em></p>

<aside class="notes"><p>unremarkable graphiti was used to confuse a ML algorithm <em>100%</em> of the time</p>
</aside>


            </section>
          <section>
              

<h2 id="embedded-industry-stuck-in-archaic-threat-model">embedded industry stuck in archaic threat model</h2>

<aside class="notes"><p>if you thought the enterprise was bad: they&rsquo;re still shipping devices with shared private keys and hard-coded passwords. we used to say things like &ldquo;well if you have physical possession then it&rsquo;s game over&rdquo; but that&rsquo;s <em>always</em> the case with IoT devices. But we have answer to that: &ldquo;secure boot&rdquo; using TPM to sign the bootloader and OS updates (without which any device can be rooted). But this is treated as an expensive add-on rather than the default</p>
</aside>


            </section>
          <section>
              

<p><img src="img/hawkbit-logo.png" alt="hawkbit" /></p>

<p><em><a href="http://www.eclipse.org/hawkbit/">http://www.eclipse.org/hawkbit/</a></em></p>

<aside class="notes"><p>existing solutions for IoT Over-the-Air updates (OTA) are mostly research projects at best (ex. Hawkbit, which depending on how you look at it is either an insecure-by-default toy, or an overcomplicated kit-of-parts)&hellip;</p>
</aside>


            </section>
          <section>
              

<p><img src="img/aws-iot.png" alt="aws-iot" /></p>

<p><em><a href="https://developer.amazon.com/de/blogs/post/Tx3828JHC7O9GZ9/Using-Alexa-Skills-Kit-and-AWS-IoT-to-Voice-Control-Connected-Devices">Robert McCauley, Alexa Blogs, 3 May 2016</a></em></p>

<aside class="notes"><p>&hellip; or require trust delegation to third parties like AWS IoT that throw-away the guarantees about provenance that put secure boot at risk</p>
</aside>


            </section>
          <section>
              

<h2 id="our-abdication-of-responsibility-invites-political-remedy">Our abdication of responsibility invites political remedy</h2>

<aside class="notes"><p>the problem with our complacency on this as an industry is that it invites someone to &ldquo;do something&rdquo; about it.</p>
</aside>


            </section>
          <section>
              

<p><img src="img/congress.jpg" alt="congress" /></p>

<p><em><a href="https://www.brookings.edu/wp-content/uploads/2016/06/congress006-1.jpg">https://www.brookings.edu/wp-content/uploads/2016/06/congress006-1.jpg</a></em></p>

<aside class="notes"><p>when these geniuses decide to act, the legal remedy is likely to be ham-fisted</p>
</aside>


            </section>
          <section>
              

<h2 id="we-have-to-do-something">&ldquo;We have to do something!&rdquo;</h2>

<h2 id="this-is-something">&ldquo;This is something&rdquo;</h2>

<h2 id="we-must-do-it">&ldquo;We must do it!&rdquo;</h2>

<aside class="notes"><p>we&rsquo;ve seen this over and over again. look at the evergreen fights over encryption, where the government thinks they can somehow make math available only to Good People</p>
</aside>


            </section>
          <section>
              

<p><img src="img/series-of-tubes.png" alt="series-of-tubes" /></p>

<p><em><a href="https://imgur.com/gallery/2mIObIu">https://imgur.com/gallery/2mIObIu</a></em></p>

<aside class="notes"><p>This is particularly a problem here in the US where virtually all our lawmakers are incompetent&hellip;</p>
</aside>


            </section>
          <section>
              

<p><img src="img/ajit-pai.png" alt="ajit-pai" /></p>

<p><em><a href="https://www.engadget.com/2017/05/18/fcc-chairman-net-neutrality-1996/">Terrence O&rsquo;Brien, engadget, 18 May 2017</a></em></p>

<aside class="notes"><p>&hellip; or they (and their minions) are entirely co-opted by hostile interests</p>
</aside>


            </section>
          <section>
              

<blockquote>
<p>Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an &ldquo;AS IS&rdquo; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License.</p>
</blockquote>

<p><em><a href="https://www.apache.org/licenses/LICENSE-2.0">https://www.apache.org/licenses/LICENSE-2.0</a></em></p>

<aside class="notes"><p>today much of our work is protected by saying &ldquo;hey we don&rsquo;t warantee this for fitness for any particular use.&rdquo; this is the Apache license but other OSS licenses are similar, as are most EULAs. we&rsquo;ve been allowed to get away with this, but that could change.</p>
</aside>


            </section>
          <section>
              

<h2 id="do-you-want-to-be-personally-liable-for-bugs-in-your-code">Do you want to be personally liable for bugs in your code?</h2>

<aside class="notes"><p>everyone here want to carry malpractice insurance?</p>
</aside>


            </section>
          <section>
              

<p><img src="img/gdpr-block.png" alt="gdpr-block" /></p>

<p><em><a href="https://apility.io/2018/05/25/gdpr-lazy-block-european-users-cloudflare-workers/">https://apility.io/2018/05/25/gdpr-lazy-block-european-users-cloudflare-workers/</a></em></p>

<aside class="notes"><p>but even in the face of reasonable privacy law like the GDPR, we&rsquo;ve seen our industry act with utter lack of professionalism. After 2 <em>years</em> notice, this &ldquo;I&rsquo;m taking my ball home&rdquo; is the best that orgs can do?</p>
</aside>


            </section>
          <section>
              

<p><img src="img/gdpr-hackernews.png" alt="gdpr-hackernews" /></p>

<p><em><a href="https://news.ycombinator.com/item?id=16954306">https://news.ycombinator.com/item?id=16954306</a></em></p>

<aside class="notes"><p>top post on The Orange Site: &ldquo;what many posters here miss is that a big group of tech people have no interest in dealing with legal matters.&rdquo; aw, poor baby! you don&rsquo;t get to be part of a world-impacting profession and pretend there are no real world consequences. childishness!</p>
</aside>


            </section>
          <section>
              

<p><img src="img/gdpr-internet-of-shit.png" alt="gdpr-internet-of-shit" /></p>

<p><em><a href="https://twitter.com/internetofshit/status/999619364541394944">https://twitter.com/internetofshit/status/999619364541394944</a></em></p>

<aside class="notes"><p>on the other hand, if this is what organizations are doing&hellip;</p>
</aside>


            </section>
          <section>
              

<p><img src="img/gdpr-unrollme.png" alt="gdpr-unrollme" /></p>

<p><em><a href="https://techcrunch.com/2018/05/05/unroll-me-to-close-to-eu-users-saying-it-cant-comply-with-gdpr/">Natasha Lomas, Tech Crunch, 5 May 2018</a></em></p>

<aside class="notes"><p>&hellip;then maybe it&rsquo;s all working as intended!</p>
</aside>


            </section>
          <section>
              

<h1 id="feature-not-bug">feature, not bug</h1>

<aside class="notes"><p>if you can&rsquo;t do your job to protect the privacy of users and have to close up shop: Mission. Accomplished.</p>
</aside>


            </section>
          <section>
              

<blockquote>
<p>Salesforce CEO Marc Benioff thinks America needs &ldquo;a national privacy law&hellip; that probably looks a lot like GDPR.</p>

<p>&ldquo;This is going to help our industry&hellip; It&rsquo;s going to set the guardrails around trust, around safety. It&rsquo;s going to provide the ability for the customers to interact with great next generation technologies in a safe way.&rdquo;</p>
</blockquote>

<p><em><a href="https://www.theregister.co.uk/2018/05/30/salesforce_q1_2019/?">Simon Sharwood, The Register, 30 May 2018</a></em></p>

<aside class="notes"><p>But it doesn&rsquo;t have to be that way. In this interview with Marc Benioff he points out this can be good for our industry. It&rsquo;ll &ldquo;set the guardrails&rdquo;</p>
</aside>


            </section>
          <section>
              

<blockquote>
<p>Benioff went on to say that as artificial intelligence is used in customer service, &ldquo;that starts to cross the line on what is trust. And that&rsquo;s where our industry really has to come forward and say we&rsquo;re going to make sure that these technologies are trust-based. And I think the Europeans definitely got that figured out.&rdquo;</p>
</blockquote>

<p><em><a href="https://www.theregister.co.uk/2018/05/30/salesforce_q1_2019/?">Simon Sharwood, The Register, 30 May 2018</a></em></p>

<aside class="notes"><p>And this is deeply important because we as an industry have failed to set those guardrails for ourselves. It&rsquo;s not too late. What can we do?</p>
</aside>


            </section>
          <section>
              

<h1 id="consent">Consent</h1>

<aside class="notes"><p>[10:00]
consent is the <em>only</em> workable guiding model when we&rsquo;re talking about relationships between individual people. I consent to being in a relationship with you. You consent to being in a relationship with me. When you decide you don&rsquo;t want to be in that relationship you can withdraw consent and I&rsquo;m supposed to respect that. We <em>hopefully</em> all understand this by now?</p>
</aside>


            </section>
          <section>
              

<h1 id="individual-vs-community-consent">Individual vs Community Consent</h1>

<aside class="notes"><p>But consent has some limits once we get a lot more people involved. Although individual consent is the basis of liberal democracy, there are some times when we decide that the will of the community overrides the consent of an individual.</p>
</aside>


            </section>
          <section>
              

<p><img src="img/vaccines.jpg" alt="vaccines" /></p>

<p><em><a href="http://www.startribune.com/sack-cartoon-vaccinations/289998831/">Steve Sack, Star Tribune, 27 Jan 2015</a></em></p>

<aside class="notes"><p>We expect everyone to pay their taxes. We ask that people are vaccinated. And the boundaries of individual vs community consent vary by culture. ex. in the EU they protect individual consent strongly&hellip;</p>
</aside>


            </section>
          <section>
              

<p><img src="img/kelo-vs-new-london.png" alt="kelo-house" /></p>

<p><em><a href="https://www.washingtonpost.com/news/volokh-conspiracy/wp/2015/05/29/the-story-behind-the-kelo-case-how-an-obscure-takings-case-came-to-shock-the-conscience-of-the-nation/">Ilya Somin, Washington Post, 29 May 2015</a></em></p>

<aside class="notes"><p>whereas in the US we have a mixed bag where businesses (which are supposed to be individuals) are often given the power of the community to override individual consent, but very little of the responsibility of consent.</p>
</aside>


            </section>
          <section>
              

<p><img src="img/tumblr-gdpr.png" alt="gdpr-tumblr" /></p>

<p><em><a href="https://twitter.com/Millstab/status/999762424994594817">https://twitter.com/Millstab/status/999762424994594817</a></em></p>

<aside class="notes"><p>this is the flaw of the consent model as a solution. we saw that side-channel information leaks make consent nearly impossible to understand. and dark UX patterns can manipulate consent. &ldquo;you agreed to this!&rdquo;</p>
</aside>


            </section>
          <section>
              

<p><img src="img/externality-definition.png" alt="externality" /></p>

<p><em><a href="https://www.google.com/search?q=externality">https://www.google.com/search?q=externality</a></em></p>

<aside class="notes"><p>we see failure of consent at play when we look at the Uber accident. the woman who was killed didn&rsquo;t consent to be part of Uber&rsquo;s experimental driving program. she wasn&rsquo;t behind the wheel. what does &ldquo;consent&rdquo; mean when other human lives are treated as an externality? aside: are these results being manipulated? the <em>canonical</em> example is pollution! what coal industry PR team infiltrated this into Google&rsquo;s results?</p>
</aside>


            </section>
          <section>
              

<h1 id="values-vary">values vary</h1>

<aside class="notes"><p>ethical definitions vary b/c values vary (ex. that balance of community vs individual consent varies significantly between Europe and the US). but we&rsquo;re not the first people to have this problem! so how do other professions solve this problem&hellip;?</p>
</aside>


            </section>
          <section>
              

<p><img src="img/sonia-1.png" alt="sonia-1" /></p>

<p><em><a href="https://twitter.com/soniagupta504/status/1011591288003575808">https://twitter.com/soniagupta504/status/1011591288003575808</a></em></p>

<aside class="notes"><p>Sonia (who was up here on stage last night): &ldquo;Lawyers and doctors enter their professions knowing, from the outset, just how heavy their burdens are. They hold human rights and life in their hands.&rdquo;</p>
</aside>


            </section>
          <section>
              

<p><img src="img/sonia-2.png" alt="sonia-2" /></p>

<p><em><a href="https://twitter.com/soniagupta504/status/1011591288003575808">https://twitter.com/soniagupta504/status/1011591288003575808</a></em></p>

<aside class="notes"><p>highly impactful professions know that their limits must be more stringent</p>
</aside>


            </section>
          <section>
              

<h2 id="with-great-power-comes-great-responsibility">&ldquo;With great power comes great responsibility&rdquo;</h2>

<aside class="notes"><p>or as all the comic book nerds in the room know it: &ldquo;with great power comes great responsibility&rdquo;</p>
</aside>


            </section>
          <section>
              

<h1 id="professional-ethics">professional ethics</h1>

<aside class="notes"><p>this is the basis of professional ethics</p>
</aside>


            </section>
          <section>
              

<h1 id="licensing-as">licensing as</h1>

<h1 id="self-regulation">self-regulation</h1>

<aside class="notes"><p>historically licensing and professional organizations (ex. AMA, AIA, ASME, bar associations) have arisen from the professions themselves rather than being imposed clumsily from the outside. government licensing requirements are typically delegated to the professional organizations</p>
</aside>


            </section>
          <section>
              

<h1 id="licensing-requires-monopoly">licensing requires monopoly</h1>

<aside class="notes"><p>professional certification w/o the consequences of regulation is basically toothless. we see lots and lots of useless rent-seeking certifications today already (CompTIA, Project Management Institute). it&rsquo;s worth considering what the side-effects of regulation-supported monopoly would be.</p>
</aside>


            </section>
          <section>
              

<blockquote>
<p>We do not believe that it is merely a coincidence that the entry and standards of practice are most strictly regulated for physicians, dentists, and veterinarians&hellip; where the costs of receiving poor services could be high or sometimes even catastrophic.</p>
</blockquote>

<p><em><a href="http://www.nber.org/papers/w10467.pdf">ref http://www.nber.org/papers/w10467.pdf</a></em></p>

<aside class="notes"><p>study by Marc Law &amp; Sukkoo Kim of National Bureau of Economic Research shows that professional organizations have acted mostly as a counter to information assymmetry rather than creating monopoly power. consumers of our services generally don&rsquo;t understand what they&rsquo;re buying, and so professional bodies provide an answer to the question of whether the product is acceptable. professional organizations ethical requirements are typically about fair dealing and bare minimum safety guidelines.</p>
</aside>


            </section>
          <section>
              

<h1 id="gatekeeping">gatekeeping</h1>

<aside class="notes"><p>&hellip; so in other words they serve as gatekeepers. well, that&rsquo;s a little problematic isn&rsquo;t it? we have enough barriers to entry in our profession as it is. we still have a <em>lot</em> of work to do, but this profession is one that can help bootstrap people firmly into the middle-class. so adding new barriers like &ldquo;you must have <em>this</em> degree&rdquo; or &ldquo;you must have this many years experience working under a licensed developer&rdquo; seem like they&rsquo;d work against the direction we want to take our industry, right?</p>
</aside>


            </section>
          <section>
              

<h1 id="shared-ethical-baseline">shared ethical baseline</h1>

<aside class="notes"><p>but an advantage of a professional organization&rsquo;s ethical framework is that it at least gives a shared baseline: right now we&rsquo;re all trying to figure this out for ourselves. and we should hold each other to task</p>
</aside>


            </section>
          <section>
              

<h1 id="what-is-to-be-done">what is to be done?</h1>

<aside class="notes"><p>[20:00]
waiting for &ldquo;the industry&rdquo; to fix it isn&rsquo;t working. we all, individually, are the industry. need to take individual action:
- influence the community
- ex. talk about this stuff!
- ex. what organizations do we work for?</p>
</aside>


            </section>
          <section>
              

<p><img src="img/microsoft-employees-ice.png" alt="microsoft-employees-ice" /></p>

<p><em><a href="https://arstechnica.com/tech-policy/2018/06/microsoft-staff-call-on-company-to-end-ice-contract/">Peter Bright, Ars Technica, 20 Jun 2018</a></em></p>

<aside class="notes"><p>we can influence the organization. we see Microsoft employees pushing for change in what customers their organization serves</p>
</aside>


            </section>
          <section>
              

<p><img src="img/google-employees-military.png" alt="google-employees-military" /></p>

<p><em><a href="https://www.reuters.com/article/uk-alphabet-defense/google-to-scrub-u-s-military-deal-protested-by-employees-source-idUKKCN1IX5YC">Paresh Dave and Heather Somerville, Reuters, 1 Jun 2018</a></em></p>

<aside class="notes"><p>we&rsquo;ve seen successful protest by Googlers motivated by their conscience</p>
</aside>


            </section>
          <section>
              

<p><img src="img/amazon-employees-law-enforcement.png" alt="amazon-employees-law-enforcement" /></p>

<p><em><a href="https://gizmodo.com/amazon-workers-demand-jeff-bezos-cancel-face-recognitio-1827037509">Kate Conger, Gizmodo, 21 Jun 2018</a></em></p>

<aside class="notes"><p>and we&rsquo;ve seen Amazon employees pushing for change</p>
</aside>


            </section>
          <section>
              

<h1 id="what-about">what about&hellip;?</h1>

<aside class="notes"><p>&ldquo;whataboutism&rdquo;: because &ldquo;values vary&rdquo; it isn&rsquo;t constructive to see e.g. Microsoftees protesting ICE but then turn around and say &ldquo;what about their military contracts?&rdquo; or &ldquo;what about that time when MSFT&rsquo;s CEO from 17 years ago said meanie-head things about open source?&rdquo; This is not helpful.</p>
</aside>


            </section>
          <section>
              

<h1 id="who-s-hiring">Who&rsquo;s hiring?</h1>

<aside class="notes"><p>the labor market for our profession gives us enormous power right now. we can push hard for better hiring policies. we can push hard for D&amp;I efforts. we can push hard for our organizations to be better</p>
</aside>


            </section>
          <section>
              

<h1 id="ensure-your-own-mask-is-secure">ensure your own mask is secure</h1>

<aside class="notes"><p>that being said, we should also cut each other some slack. you don&rsquo;t know much about the circumstances of any particular person. so while we should be holding each other to task, individuals need to make their own decisions about where they work and that doesn&rsquo;t make them The Enemy. (unless they work at Palantir, just sayin&rsquo;)</p>
</aside>


            </section>
          <section>
              

<h1 id="direct-action">direct action</h1>

<aside class="notes"><p>what is available to everyone regardless of our work conditions, and perhaps more effective than anything else we can do, is direct action at an engineering level. what choices do we make as technologists?</p>
</aside>


            </section>
          <section>
              

<h2 id="immutable">Immutable?</h2>

<p><img src="img/Kafka.png" alt="kafka" /></p>

<p><a href="https://vision.cloudera.com/apache-kafka-a-platform-for-real-time-data-streams-part-1/">Jay Kreps, Cloudera, 11 June 2015</a></p>

<aside class="notes"><p>the pipelines we use to ingest data in many orgs are using immutable event stores</p>
</aside>


            </section>
          <section>
              

<h2 id="immutable-1">&ldquo;Immutable&rdquo;</h2>

<p><img src="img/Kafka-per-user.png" alt="kafka" /></p>

<aside class="notes"><p>&hellip; per user encryption keys can be used to allow immutable infrastructures that are compatible with the &ldquo;right to be forgotten&rdquo;</p>
</aside>


            </section>
          <section>
              

<pre><code class="language-python">def should_brake(road):
    if road.contains(object.HUMAN):
        print(&quot;oh shit!&quot;)
        # TODO: this is causing erratic driving on false
        # positive detection. Uncomment this once we have
        # that solved. Someone should remind the field
        # engineers to tell the test drivers they need to
        # pay attention to the road.
        # return True
    return False
</code></pre>

<aside class="notes"><p>we need to take responsibility for quality proportional to the risks involved with the software. this is obvious in self-driving cars</p>
</aside>


            </section>
          <section>
              

<h1 id="ml-models-are-state">ML models are state</h1>

<aside class="notes"><p>our entire industry has unified around our worries about statefulness, but&hellip;
ML is the ultimate stateful application. you&rsquo;re using software to generate these software models &ndash; the entire application is a side-effect! how can we influence those side-effects?</p>
</aside>


            </section>
          <section>
              

<p><img src="img/machine-learning-tech-debt.png" alt="machine-learning-tech-debt" /></p>

<p><em><a href="https://ai.google/research/pubs/pub43146">https://ai.google/research/pubs/pub43146</a></em></p>

<aside class="notes"><p>we desperately need better tooling to design-out unexpected behaviors in ML. The hidden feedback loops and undeclared consumers, and entangled data dependencies represent a side-channel source of technical debt. and it&rsquo;s the worst kind of technical debt &ndash; it&rsquo;s &ldquo;shadow debt&rdquo; that&rsquo;s taken on unknowingly.</p>
</aside>


            </section>
          <section>
              

<blockquote>
<p>To make great products:</p>

<p>do machine learning like the great engineer you are, not like the great machine learning expert you aren&rsquo;t.</p>

<p>&hellip;</p>

<p>Insofar as well-being and company health is concerned, human judgement is required to connect any machine learned objective to the nature of the product you are selling and your business plan.</p>
</blockquote>

<p><em><a href="https://developers.google.com/machine-learning/rules-of-ml/">Martin Zinkevich (Google), Rules of Machine Learning: Best Practices for ML Engineering</a></em></p>


            </section>
          <section>
              

<h1 id="sql-ml">SQL &gt; ML</h1>

<aside class="notes"><p>first rule should always be: why are we choosing ML over some well-tuned SQL or other algorithm? are we choosing to use a method with chaotic feedback mechanisms instead of something that&rsquo;s simple and deterministic simply because of resume-driven development?</p>
</aside>


            </section>
          <section>
              

<h2 id="models-should-be-testable-and-human-interpretable">models should be testable and human-interpretable</h2>

<aside class="notes"><p>simple linear or logrithimic regression models are easier to debug, calibrate, and avoid feedback loops than models that try to optimize their own accuracy. combine features in human-understandable ways, remove unused features (which represent both technical debt and side-channel opportunities), quantify any observed undesirable behavior and build tests for it.</p>
</aside>


            </section>
          <section>
              

<h1 id="align-training-data-with-real-world-demographics">align training data with real world demographics</h1>

<aside class="notes"><p>choose ML model inputs that reflect the population. this is win-win: align engineering ethics with business needs (ex. &ldquo;if we pick machine vision training data that reflects real demographics, we can avoid the embarassment for our organization of having to explain why our software acts racist.&rdquo;)</p>
</aside>


            </section>
          <section>
              

<h1 id="technical-leadership">technical leadership</h1>

<aside class="notes"><p>we can&rsquo;t rely on project managers or business analysts to take the lead on designing our systems ethically because they simply may not understand the side-effects. you&rsquo;re the technical professional: they&rsquo;re expecting you to take the lead on this!</p>
</aside>


            </section>
          <section>
              

<h1 id="best-practice-is">&ldquo;best practice is&hellip;&rdquo;</h1>

<aside class="notes"><p>&ldquo;the best thing about best practices is there are so many of them to choose from.&rdquo; you don&rsquo;t need to ask permission from your business analysts and project managers on opinions that are purely technical</p>
</aside>


            </section>
          <section>
              

<p><img src="img/jira-clock-puncher.png" alt="jira-clock-puncher" />
<span class='fragment '
  >
  <img src="img/jira-clock-puncher.png" alt="jira-clock-puncher" />
</span>
<span class='fragment '
  >
  <img src="img/jira-clock-puncher.png" alt="jira-clock-puncher" />
</span>
<span class='fragment '
  >
  <img src="img/jira-clock-puncher.png" alt="jira-clock-puncher" />
</span>
<span class='fragment '
  >
  <img src="img/jira-clock-puncher.png" alt="jira-clock-puncher" />
</span>
<span class='fragment '
  >
  <img src="img/jira-clock-puncher.png" alt="jira-clock-puncher" />
</span>
<span class='fragment '
  >
  <img src="img/jira-clock-puncher.png" alt="jira-clock-puncher" />
</span>
<span class='fragment '
  >
  <img src="img/jira-clock-puncher.png" alt="jira-clock-puncher" />
</span>
<span class='fragment '
  >
  <img src="img/jira-clock-puncher.png" alt="jira-clock-puncher" />
</span>
<span class='fragment '
  >
  <img src="img/jira-clock-puncher.png" alt="jira-clock-puncher" />
</span>
<span class='fragment '
  >
  <img src="img/jira-clock-puncher.png" alt="jira-clock-puncher" />
</span>
<span class='fragment '
  >
  <img src="img/jira-clock-puncher.png" alt="jira-clock-puncher" />
</span>
<span class='fragment '
  >
  <img src="img/jira-clock-puncher.png" alt="jira-clock-puncher" />
</span>
<span class='fragment '
  >
  <img src="img/jira-clock-puncher.png" alt="jira-clock-puncher" />
</span>
<span class='fragment '
  >
  <img src="img/jira-clock-puncher.png" alt="jira-clock-puncher" />
</span></p>

<aside class="notes"><p>formal Agile methodologies contribute to the problem. &ldquo;act ethically&rdquo; doesn&rsquo;t have any story points in this epic. But you&rsquo;re supposed to be a professional, not a JIRA clock puncher. act like it!</p>
</aside>


            </section>
          <section>
              

<blockquote>
<p>We aren&rsquo;t a craft anymore. We might feel like artisans with laptops but what we produce could potentially be in front of a significant chunk of the human race by lunchtime. We&rsquo;re not hand-crafting dovetail joints here.</p>
</blockquote>

<p><em><a href="https://www.theregister.co.uk/2018/03/01/ethics_yeah_thats_great_but_do_they_scale/">Anne Currie, The Register, 1 Mar 2018</a></em></p>

<aside class="notes"><p>leave you with&hellip;</p>
</aside>


            </section>
          <section>
              

<h1 id="let-s-get-to-work">let&rsquo;s get to work</h1>

            </section>
          
  </div>
</div>


    <script type="application/json" id="reveal-hugo-page-params">{"custom_theme":"./css/devopsdays.css","highlight_theme":"solarized-dark"}</script>
    <script type="application/json" id="reveal-hugo-site-params">{"theme":"beige"}</script>
    <script type="application/json" id="reveal-hugo-defaults">{"center":true,"controls":true,"highlight_theme":"default","history":true,"progress":true,"theme":"black","transition":"slide"}</script>
    <script type="text/javascript">
      window.revealHugoDependencies = {
        dependencies: [
          { src: './lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: './plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
          { src: './plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
          { src: './plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
          { src: './plugin/zoom-js/zoom.js', async: true },
          
          { src: 'reveal-hugo\/plugin\/notes\/notes.js', async: true }
      ]};
    </script>
    <script src="./lib/js/head.min.js"></script>
    <script src="./js/reveal.js"></script>
    <script type="text/javascript">
      
      
      function camelize(map) {
        if (map) {
          Object.keys(map).forEach(function(k) {
            newK = k.replace(/(\_\w)/g, function(m) { return m[1].toUpperCase() });
            if (newK != k) {
              map[newK] = map[k];
              delete map[k];
            }
          });
        }
        return map;
      }
      
      var revealHugoPageParams = JSON.parse(document.getElementById('reveal-hugo-page-params').innerHTML);
      var revealHugoSiteParams = JSON.parse(document.getElementById('reveal-hugo-site-params').innerHTML);
      var revealHugoDefaults = JSON.parse(document.getElementById('reveal-hugo-defaults').innerHTML);

      
      var options = Object.assign(
        camelize(revealHugoDefaults),
        camelize(revealHugoSiteParams),
        camelize(revealHugoPageParams),
        revealHugoDependencies);
      Reveal.initialize(options);

    </script>
    
  </body>
</html>
